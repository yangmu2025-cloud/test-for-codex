# 论文抓取工具（Paper Crawler）

一个以**论文检索与抓取**为核心的轻量项目，用于从目标学术站点获取论文元数据，并为后续清洗、去重、入库与分析提供基础数据。

> 当前仓库以 `paper_crawler.rar` 作为抓取模块的打包产物，同时保留 `create_github_repo.sh` 作为仓库发布辅助脚本。

---

## 项目定位

本项目的主线能力是：

1. 按关键词或主题进行论文检索。
2. 抓取论文基础信息（如标题、作者、摘要、来源、发布时间等）。
3. 对抓取结果进行结构化输出，供后续分析流程使用。

围绕主线，项目内容被整理为：

- **核心模块**：`paper_crawler.rar`（论文抓取能力的打包文件）。
- **辅助工具**：`create_github_repo.sh`（仅用于将当前仓库快速发布到 GitHub，不参与抓取逻辑）。
- **说明文档**：`README.md`（聚焦抓取功能，移除重复说明与偏离主线内容）。

---

## 目录结构

```text
.
├── paper_crawler.rar       # 核心：论文抓取模块（打包文件）
├── create_github_repo.sh   # 辅助：一键创建并发布 GitHub 仓库
├── README.md               # 项目文档（本文件）
└── LICENSE
```

---

## 功能主线（论文抓取）

### 输入
- 关键词 / 主题词
- 可选过滤条件（时间范围、来源站点、学科方向等）

### 处理
- 访问目标页面或接口
- 解析论文条目
- 抽取关键信息字段
- 去重与基础规范化（按标题、DOI、来源链接等维度）

### 输出
- 结构化结果（推荐 JSON / CSV）
- 可用于：文献综述、选题调研、趋势分析、知识库构建

---

## 推荐工作流

1. **准备抓取参数**：定义关键词、时间范围、抓取深度。
2. **执行抓取任务**：运行论文抓取模块并落盘结果。
3. **数据质检与去重**：清理空字段、重复条目和异常记录。
4. **下游消费**：导入数据库、可视化分析或生成报告。

---

## 当前仓库说明

由于核心抓取模块目前以压缩包形式存放，建议后续按以下方向演进：

- 将 `paper_crawler.rar` 解包并纳入可追踪源码目录；
- 为抓取模块补充独立的使用说明与样例配置；
- 增加可复现的测试命令和最小示例数据；
- 将发布脚本与抓取模块文档解耦，保持“核心能力优先”的项目结构。

---

## GitHub 发布辅助（可选）

`create_github_repo.sh` 仅用于仓库发布，不属于论文抓取主流程。

用法：

```bash
./create_github_repo.sh <repo-name> [public|private] [description]
```

示例：

```bash
./create_github_repo.sh paper-crawler private "paper crawler project"
```

依赖：`bash`、`git`、`curl`、`jq`，并需设置 `GITHUB_TOKEN`。

---

## 许可证

本项目基于 `LICENSE` 文件中声明的许可证发布。
